# 📘 Click, Sync, Search: Real-Time Customer Search with Elasticsearch, Logstash & MySQL

This project demonstrates how to build a real-time, lightning-fast customer search system by syncing MySQL data into Elasticsearch using Logstash. It covers full-text search, shard replication, inverted indexing, and API querying using Postman.

---

## 🛠️ Tech Stack
- **MySQL 8.0** – Relational source
- **Elasticsearch 8.3.1** – Search & analytics engine
- **Logstash 8.3.1** – ETL pipeline
- **Kibana 8.3.1** – Visualization dashboard
- **Postman** – For testing REST queries

---

## 📥 Sample Data Setup

Download `mysqlsampledatabase.sql` from:  
🔗 https://github.com/hhorak/mysql-sample-db/blob/master/mysqlsampledatabase.sql

### Import Steps:
1. Open MySQL Workbench
2. Go to **Server > Data Import**
3. Select `Import from Self-Contained File` and choose the SQL
4. Set target schema to `classicmodels`
5. Click **Start Import**

Run this to verify:
```sql
SELECT * FROM classicmodels.customers;
```

---

## 🚀 Elasticsearch Cluster Setup

### Step 1: Start the First Node
```bash
bin/elasticsearch
```

You’ll get:
- Enrollment token
- Elastic superuser password
- TLS certificates

If password is lost, reset with:
```bash
bin/elasticsearch-reset-password -u elastic
```

### Step 2: Start Other Nodes
```bash
bin/elasticsearch --enrollment-token <paste-token-here>
```

Verify nodes:
```http
GET _cat/nodes?v
```

---

## 📊 Kibana Setup

```bash
bin/kibana.bat
```

Visit:
```
http://localhost:5601/?code=<auto-generated-code>
```

---

## 🔁 Shard Allocation
<table>
  <tr>
    <td><img src="https://github.com/user-attachments/assets/19d72670-80ee-4aae-98dc-a24174f055dc" width="300"/></td>
    <td><img src="https://github.com/user-attachments/assets/ab001e9d-6184-4233-bcb6-e84be4beb2a4" width="300"/></td>
  </tr>
  <tr>
    <td><img src="https://github.com/user-attachments/assets/ab68173a-c37d-4ffb-a4ef-89f327e5dd83" width="300"/></td>
    <td><img src="https://github.com/user-attachments/assets/7597e662-c06d-4970-959d-2ec428798def" width="300"/></td>
  </tr>
</table>


Use DevTools:
```http
GET _cat/shards?v
```

Example output:
```
customer_idx 0 p STARTED 122 69.1kb 127.0.0.1 node-2
customer_idx 0 r STARTED 122 92.6kb 127.0.0.1 node-1
```

### Explanation:
- `p`: Primary shard
- `r`: Replica shard

### Create Index with Shards:
```json
PUT customer_idx
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 1
  }
}
```

---

## 🧠 Inverted Index Explained
![image](https://github.com/user-attachments/assets/6f706ca0-4ac4-447a-9abe-c47cfb36fb74)

Instead of scanning every document, Elasticsearch builds a **token → docID** map.  
Searching for `"Rosa"` will directly return the matching doc ID(s) using inverted index.

### Token Table Example:
| Token | Documents |
|-------|-----------|
| rosa  | 486       |
| usa   | 486, 298  |

### Advantages:
- No full scan
- Blazing fast text search
- Ideal for large datasets

---

## 🔌 Logstash Pipeline
![image](https://github.com/user-attachments/assets/a4da022b-77ee-48f1-ab68-c280876d16de)

`customer_config.conf` example:
```ruby
input {
    jdbc {
	 clean_run => true
        jdbc_driver_library => "C:\Users\Hasnain Ahmed Shaikh\Documents\ELK\mysql-connector-java-8.0.30.jar"
        jdbc_driver_class => "com.mysql.jdbc.Driver"
        jdbc_connection_string => "jdbc:mysql://localhost:3306/classicmodels"
        jdbc_user => "root"
	jdbc_password => "root"
        #schedule => "* * * * *"
        statement => "select * from classicmodels.customers"
        #use_column_value => true
        #tracking_column => "category"
	#tracking_column => "last_update"
    }
}
filter {
  mutate {
    remove_field => ["@version"]
  }
}
output{
    elasticsearch {	
        hosts => ["https://localhost:9200/"]
        index => "customer_idx"
	document_id => "%{customernumber}"  # 👈 Sets _id using customerNumber
	user => "elastic"
	password => "Kvi1IAybXQI9LMNhpGqp"
	ssl => true
	ssl_certificate_verification => false
    }
   
}
```

---

## 🔄 Enable CDC (Change Data Capture)
![image](https://github.com/user-attachments/assets/3dccba32-ebe7-452f-8411-a3e93f7e7126)

Add a column to MySQL:
```sql
ALTER TABLE customers ADD COLUMN last_update TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP;
```

Update Logstash input:
```ruby
input {
  jdbc {
    schedule => "* * * * *"
    statement => "SELECT * FROM customers WHERE last_update > :sql_last_value"
    use_column_value => true
    tracking_column => "last_update"
    tracking_column_type => "timestamp"
    record_last_run => true
    last_run_metadata_path => "./last_run/customers_last_run.yml"
  }
}
```

---

## 🧪 Search with Postman
![image](https://github.com/user-attachments/assets/fd5447ca-30b6-4af6-8a5b-c50c51178064)
POST request:
```
https://localhost:9200/customer_idx/_search
```

Body:
```json
{
  "query": {
    "query_string": {
      "query": "Rosa"
    }
  }
}
```




---

## ✅ Next Steps
- Enable scheduled syncing
- Build Kibana dashboards
- Explore aggregations
- Add fuzzy or wildcard search

---

Made with 💡 by Hasnain Ahmed Shaikh
